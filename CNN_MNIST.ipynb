{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat Nov 30 13:36:16 2019\n",
    "\n",
    "@author: Talha Ilyas\n",
    "\"\"\"\n",
    "# Import necessary libraries\n",
    "import idx2numpy\n",
    "import numpy\n",
    "import time, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    # initialise the neural network\n",
    "    def __init__(self, filter_1, filter_2, h1nodes, h2nodes, opnodes, bias_hid2op, bias_hid1hid2, bias_iphid1, momentum, BatchSize, learningrate, decay):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        filter_1 : filter size for 1st conv layer [type = tuple] e.g. ->  (5,5)\n",
    "        filter_2 : filter size for 2nd conv layer [type = tuple] e.g. ->  (5,5)\n",
    "        input_nodes : Number of nodes in 1st hidden layer [type = int] e.g. -> 200\n",
    "        h1nodes : Number of nodes in 2nd hidden layer [type = int] e.g. -> 200\n",
    "        h2nodes : Number of nodes in 3rd hidden layer [type = int] e.g. -> 200\n",
    "        opnodes : Number of nodes in ouput layer [type = int] equal to number of classes;\n",
    "                  in case of CIFAR-10 and MNIST -> 10\n",
    "        bias_hid2op : bias vector len. equal to output_nodes\n",
    "        bias_hid1hid2 : bias vector len. equal to nodes_in_2nd_hidden_layer\n",
    "        bias_iphid1 : bias vector len. equal to nodes_in_1st_hidden_layer\n",
    "        momentum : a hyperparameter for gradient descent [type = float] e.g. ->  0.9\n",
    "        BatchSize : for how many times to iterate over whole data [type = int] e.g. ->  50\n",
    "        learningrate : a hyperparameter for gradient descent [type = float] e.g. ->  0.01\n",
    "        decay : a hyperparameter for learning rate decay [type = float] e.g. ->  0.0001\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        '''\n",
    "        # set number of nodes in each input, hidden, output layer\n",
    "        #Two Conv Filters\n",
    "        self.filter1_h, self.filter1_w = filter_1\n",
    "        self.filter2_h, self.filter2_w = filter_2\n",
    "\n",
    "        self.ip_nodes = 16\n",
    "        self.h1_nodes = h1nodes\n",
    "        self.h2_nodes = h2nodes\n",
    "        self.op_nodes = opnodes\n",
    "\n",
    "        self.bias_h2op = bias_hid2op\n",
    "        self.bias_h1h2 = bias_hid1hid2\n",
    "        self.bias_iph1 = bias_iphid1\n",
    "        \n",
    "        self.batch_size = BatchSize\n",
    "        #Momentum\n",
    "        self.beta = momentum\n",
    "        \n",
    "        self.Vdw_h2_op = 0\n",
    "        self.Vdw_h1_h2 = 0\n",
    "        self.Vdw_ip_h1 = 0\n",
    "        \n",
    "        self.Vdb_h2_op = 0\n",
    "        self.Vdb_h1_h2 = 0\n",
    "        self.Vdb_ip_h1 = 0\n",
    "\n",
    "        #Guassian Normal Distribution pow() means deviation in values is between +- h2_nodes**-0.5 with mean=0\n",
    "        self.w_c1 = numpy.random.normal(0.0, pow(self.filter1_h, -0.5),(self.filter1_h, self.filter1_w))\n",
    "        self.w_c2 = numpy.random.normal(0.0, pow(self.filter2_h, -0.5),(self.filter2_h, self.filter2_w))\n",
    "        # Linking weights\n",
    "        #Guassian Normal Distribution pow() means deviation in values is between +- h2_nodes**-0.5 with mean=0\n",
    "        self.w_ip_h1 = numpy.random.normal(0.0, pow(self.h1_nodes, -0.5),(self.h1_nodes, self.ip_nodes))\n",
    "        self.w_h1_h2 = numpy.random.normal(0.0, pow(self.h2_nodes, -0.5),(self.h2_nodes, self.h1_nodes))\n",
    "        self.w_h2_op = numpy.random.normal(0.0, pow(self.op_nodes, -0.5),(self.op_nodes, self.h2_nodes))\n",
    "        #Linking Biases\n",
    "        #Guassian Normal Distribution pow() means deviation in values is between +- h2_nodes**-0.5 with mean=0\n",
    "        self.bias_h2_op = numpy.random.normal(0.0, pow(self.bias_h2op, -0.5),(self.bias_h2op, 1))\n",
    "        self.bias_h1_h2 = numpy.random.normal(0.0, pow(self.bias_h1h2, -0.5),(self.bias_h1h2, 1))\n",
    "        self.bias_ip_h1 = numpy.random.normal(0.0, pow(self.bias_iph1, -0.5),(self.bias_iph1, 1))\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = learningrate\n",
    "        self.lr_decay = decay\n",
    "        \n",
    "        # activation function is the sigmoid function\n",
    "        self.Sigm = lambda x: scipy.special.expit(x)\n",
    "        pass\n",
    "     #--------------------------Derivative of Activation Functions----------------------------\n",
    "    def d_Sigm(self,x):\n",
    "        self.x = x * (1 - x)\n",
    "        return self.x\n",
    "    #---------------Single Layer/channel  Conv 1st filter----------------------------------------------------\n",
    "    def sc_conv(self, image, flter):\n",
    "        X = flter\n",
    "        Y = image\n",
    "        k,l=X.shape\n",
    "        i,j=Y.shape\n",
    "        Op = numpy.zeros((i-k+1,i-k+1))\n",
    "        for b in range(j-l+1): \n",
    "            for a in range(i-k+1):\n",
    "                Op[b,a] = numpy.sum(numpy.multiply(X,Y[b:k+b,a:a+k]),axis=(0,1))\n",
    "        return Op\n",
    "    #---------------Single filter pooling---------------------------------\n",
    "    def sf_pooling(self, Op):\n",
    "        op = Op\n",
    "        c,d = op.shape\n",
    "        e = c-c//2\n",
    "        pool = numpy.zeros((e,e))\n",
    "        locations = numpy.zeros((e,e))\n",
    "        for g in range(c-e):\n",
    "            for f in range(c-e):\n",
    "                pool[g,f] = numpy.max(op[g*2:g+g+2,f*2:f+f+2])\n",
    "                locations[g,f] = numpy.argmax(Op[g*2:g+g+2,f*2:f+f+2])\n",
    "        return locations,pool\n",
    "    #------------------------Back Pool Single Layer/Channel---------------------------\n",
    "    def b_sf_pooling(self, before_pooling_shape,locations,pool_values):\n",
    "        cache1 = pool_values\n",
    "        cache2 = before_pooling_shape.shape\n",
    "        cache3 = locations.astype(int)\n",
    "        b_Op = numpy.zeros(cache2)\n",
    "        t,u = cache1.shape\n",
    "        for j in range(u):\n",
    "            for i in range(t):\n",
    "               a = cache3[j,i]\n",
    "               numpy.put(b_Op[j*2:j+j+2,i*2:i+i+2],a, cache1[j,i])\n",
    "        return b_Op\n",
    "    #------------------------Back Conv Single Layer/Channel---------------------------\n",
    "    def b_sc_conv(self, b_pool_op, image):  \n",
    "        X = b_pool_op\n",
    "        Y = image\n",
    "        k,l=X.shape\n",
    "        i,j=Y.shape\n",
    "        f_g = numpy.zeros((i-k+1,i-k+1))\n",
    "        for b in range(j-l+1):\n",
    "            for a in range(i-k+1):\n",
    "                f_g[b,a] = numpy.sum(numpy.multiply(X,Y[b:k+b,a:a+k]),axis=(0,1))\n",
    "        return f_g\n",
    "    #------------------------------------------------------------ Train the CNN------------------------------------\n",
    "    def train(self, inputs_list, targets_list,iteration,epoch):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2)\n",
    "        targets = numpy.array(targets_list, ndmin=2)\n",
    "        t = iteration\n",
    "        #e = epoch\n",
    "        #--------------Learning Rate Decay--------------------\n",
    "        lr = self.lr\n",
    "        lr *=  (1. / (1. + (self.lr_decay * t)))\n",
    "        #------Conv 1 ------\n",
    "        conv1_op = self.sc_conv(inputs,self.w_c1)\n",
    "        #------activate----\n",
    "        conv1_opA = self.Sigm(conv1_op)\n",
    "        #------Pool 1------\n",
    "        locations1,pool1_op = self.sf_pooling(conv1_opA)\n",
    "        \n",
    "        #---------Conve 2-----\n",
    "        conv2_op = self.sc_conv(pool1_op,self.w_c2)\n",
    "        #-----Activate------\n",
    "        conv2_opA = self.Sigm(conv2_op)\n",
    "        #-----Pool 2------\n",
    "        locations2,pool2_op = self.sf_pooling(conv2_opA)\n",
    "        #-----Flattening----------\n",
    "        t = pool2_op.flatten()\n",
    "        CNN_op = t.reshape(16,1)\n",
    "        \n",
    "        X_h1 = numpy.add(numpy.dot(self.w_ip_h1, CNN_op) , self.bias_ip_h1)\n",
    "        O_h1 = self.Sigm(X_h1)\n",
    "        # calculate signals into hidden layer\n",
    "        X_h2 = numpy.add(numpy.dot(self.w_h1_h2, O_h1) , self.bias_h1_h2)\n",
    "        O_h2 = self.Sigm(X_h2)\n",
    "        # calculate signals into final output layer\n",
    "        X_op = numpy.add(numpy.dot(self.w_h2_op, O_h2) , self.bias_h2_op)\n",
    "        O_op = self.Sigm(X_op)\n",
    "        # output layer error is the (target - actual)\n",
    "        errors_op = targets - O_op\n",
    "        errors_h2 = numpy.dot(self.w_h2_op.T, errors_op)\n",
    "        errors_h1 = numpy.dot(self.w_h1_h2.T, errors_h2)\n",
    "        errors_ip = numpy.dot(self.w_ip_h1.T, errors_h1)\n",
    "        errors_ipR = errors_ip.reshape(4,4)\n",
    "        #errors_ipR = numpy.rot90(errors_ipR,k=2)\n",
    "        \n",
    "        errors_ipRB = self.b_sf_pooling(conv2_op, locations2, errors_ipR)\n",
    "        errors_c2 = errors_ipRB\n",
    "        errors_c2P = numpy.pad(errors_c2,(4,4),'constant', constant_values=(0))\n",
    "        w_c2F = numpy.rot90(self.w_c2,k=2)\n",
    "        errors_c1 = self.sc_conv(errors_c2P, w_c2F)\n",
    "        #errors_c1 = numpy.rot90(errors_c1,k=2)\n",
    "        errors_c1B = self.b_sf_pooling(conv1_op, locations1, errors_c1)\n",
    "        #errors_c1B = numpy.rot90(errors_c1B, k=2)\n",
    "        \n",
    "        self.dw_h2_op = numpy.dot((errors_op * self.d_Sigm(O_op)), numpy.transpose(O_h2))\n",
    "        self.dw_h1_h2 = numpy.dot((errors_h2 * self.d_Sigm(O_h2)), numpy.transpose(O_h1))\n",
    "        self.dw_ip_h1 = numpy.dot((errors_h1 * self.d_Sigm(O_h1)), numpy.transpose(CNN_op))\n",
    "        #For Biases\n",
    "        self.db_h2_op = (numpy.sum(errors_op *self.d_Sigm(O_op))) / self.batch_size\n",
    "        self.db_h1_h2 = (numpy.sum(errors_h2 *self.d_Sigm(O_h2))) / self.batch_size\n",
    "        self.db_ip_h1 = (numpy.sum(errors_h1 *self.d_Sigm(O_h1))) / self.batch_size\n",
    "        \n",
    "        self.Vdw_h2_op =  beta*self.Vdw_h2_op +(1-beta)*self.dw_h2_op\n",
    "        self.Vdw_h1_h2 =  beta*self.Vdw_h1_h2 +(1-beta)*self.dw_h1_h2\n",
    "        self.Vdw_ip_h1 =  beta*self.Vdw_ip_h1 +(1-beta)*self.dw_ip_h1\n",
    "        \n",
    "        self.Vdb_h2_op =  beta*self.Vdb_h2_op +(1-beta)*self.db_h2_op\n",
    "        self.Vdb_h1_h2 =  beta*self.Vdb_h1_h2 +(1-beta)*self.db_h1_h2\n",
    "        self.Vdb_ip_h1 =  beta*self.Vdb_ip_h1 +(1-beta)*self.db_ip_h1\n",
    "        #------Back Conv----------------------------------------------------------------MayBe---------------------------------------------\n",
    "        f_g2 = errors_c2 * self.d_Sigm(conv2_opA)\n",
    "        f_g1 = errors_c1B * self.d_Sigm(conv1_opA)\n",
    "        f_grad2 = self.b_sc_conv(f_g2, pool1_op)\n",
    "        f_grad1 = self.b_sc_conv(f_g1, inputs)\n",
    "        \n",
    "        #--------------------------------------------------------------------------------       \n",
    "        self.w_c2 += lr * f_grad2\n",
    "        self.w_c1 += lr * f_grad1\n",
    "        # update the weights for the links between the hidden and output layers\n",
    "        self.w_h2_op += lr * self.Vdw_h2_op\n",
    "        self.w_h1_h2 += lr * self.Vdw_h1_h2\n",
    "        self.w_ip_h1 += lr * self.Vdw_ip_h1\n",
    "        \n",
    "        # update the biases for the links between the hidden and output layers\n",
    "        self.bias_h2_op += lr * self.Vdb_h2_op\n",
    "        self.bias_h1_h2 += lr * self.Vdb_h1_h2\n",
    "        self.bias_ip_h1 += lr * self.Vdb_ip_h1\n",
    "        \n",
    "        return errors_op,lr\n",
    "# query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = numpy.array(inputs_list, ndmin=2)\n",
    "        #------Conv 1 ------\n",
    "        conv1_op = self.sc_conv(inputs,self.w_c1)\n",
    "        #------activate----\n",
    "        conv1_opA = self.Sigm(conv1_op)\n",
    "        #------Pool 1------\n",
    "        locations1,pool1_op = self.sf_pooling(conv1_opA)\n",
    "        \n",
    "        #---------Conve 2-----\n",
    "        conv2_op = self.sc_conv(pool1_op,self.w_c2)\n",
    "        #-----Activate------\n",
    "        conv2_opA = self.Sigm(conv2_op)\n",
    "        #-----Pool 2------\n",
    "        locations2,pool2_op = self.sf_pooling(conv2_opA)\n",
    "        #-----Flattening----------\n",
    "        t = pool2_op.flatten()\n",
    "        CNN_op = t.reshape(16,1)\n",
    "\n",
    "        # calculate signals into 1st hidden layer\n",
    "        X_h1 = numpy.add(numpy.dot(self.w_ip_h1, CNN_op) , self.bias_ip_h1)\n",
    "        O_h1 = self.Sigm(X_h1)\n",
    "        # calculate signals into 2nd hidden layer\n",
    "        X_h2 = numpy.add(numpy.dot(self.w_h1_h2, O_h1) , self.bias_h1_h2)\n",
    "        O_h2 = self.Sigm(X_h2)\n",
    "        # calculate signals into final output layer\n",
    "        X_op = numpy.add(numpy.dot(self.w_h2_op, O_h2) , self.bias_h2_op)\n",
    "        O_op = self.Sigm(X_op)\n",
    "        return O_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of training and testing instances you want ot run your algorithm\n",
    "X = 60000     #Training Instances\n",
    "Y = 10000     #Testing Instances\n",
    "# number of input, hidden and output nodes\n",
    "filter_1 = (5,5)\n",
    "filter_2 = (5,5)\n",
    "nodes_in_1st_hidden_layer = 12\n",
    "nodes_in_2nd_hidden_layer = 12\n",
    "output_nodes = 10\n",
    "# learning rate\n",
    "learning_rate = 0.01\n",
    "decay = 0.0001\n",
    "#ADAM 1st and 2nd Moments\n",
    "beta = 0.9\n",
    "#For scaling Bias Updates\n",
    "Global_Batchsize = 1\n",
    "#Epochs -or- iteration\n",
    "epochs = 30\n",
    "#Data Aranging for NN Class\n",
    "hidden1_nodes = nodes_in_1st_hidden_layer\n",
    "hidden2_nodes = nodes_in_2nd_hidden_layer\n",
    "bias_iph1 = nodes_in_1st_hidden_layer\n",
    "bias_h1h2 = nodes_in_2nd_hidden_layer\n",
    "bias_h2op = output_nodes\n",
    "# create instance of neural network\n",
    "n = CNN(filter_1, filter_2,hidden1_nodes,hidden2_nodes, output_nodes, bias_h2op,\n",
    "        bias_h1h2, bias_iph1, beta, Global_Batchsize, learning_rate, decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------Loading Training Data-----------------------------------\n",
    "training_data_ip = idx2numpy.convert_from_file('data/train-images.idx3-ubyte')\n",
    "training_data_label = idx2numpy.convert_from_file('data/train-labels.idx1-ubyte')\n",
    "\n",
    "inputs_train = ((training_data_ip / 255.0) * 0.99) + 0.01\n",
    "inputs_train = numpy.reshape(inputs_train, (60000,784))\n",
    "inputs_small = inputs_train[0:X,:]\n",
    "inputs_small = numpy.reshape(inputs_small,(X,784))   \n",
    "targets_train = numpy.zeros([10,60000]) + 0.01\n",
    "for c in range(len(training_data_label)):\n",
    "   r = training_data_label[c]\n",
    "   targets_train[r][c] = 0.99 \n",
    "pass\n",
    "targets_small = targets_train[:,0:X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------Testing------------------------------------------\n",
    "test_data_ip = idx2numpy.convert_from_file('data/t10k-images.idx3-ubyte.idx3-ubyte')\n",
    "test_data_label = idx2numpy.convert_from_file('data/t10k-labels.idx1-ubyte')\n",
    "\n",
    "\n",
    "inputs_test = ((test_data_ip / 255.0) * 0.99) + 0.01\n",
    "inputs_test = numpy.reshape(inputs_test, (10000,784))\n",
    "inputs_stest = inputs_test[0:Y,:]\n",
    "targets_test = numpy.zeros([10,10000]) + 0.01\n",
    "for c1 in range(len(test_data_label)):\n",
    "       r1 = test_data_label[c1]\n",
    "       targets_test[r1][c1] = 0.99\n",
    "targets_stest = targets_train[:,0:Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   Epoch 1: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:25<00:00, 155.47it/s]\n",
      "Validating Epoch 1: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:45<00:00, 220.50it/s]\n",
      "\n",
      "After 1  Epoch Accuracy= 11.35 %\n",
      "\n",
      "Training   Epoch 2: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:16<00:00, 159.50it/s]\n",
      "Validating Epoch 2: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 225.30it/s]\n",
      "\n",
      "After 2  Epoch Accuracy= 11.35 %\n",
      "\n",
      "Training   Epoch 3: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.39it/s]\n",
      "Validating Epoch 3: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.11it/s]\n",
      "\n",
      "After 3  Epoch Accuracy= 20.74 %\n",
      "\n",
      "Training   Epoch 4: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:12<00:00, 161.23it/s]\n",
      "Validating Epoch 4: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.70it/s]\n",
      "\n",
      "After 4  Epoch Accuracy= 40.300000000000004 %\n",
      "\n",
      "Training   Epoch 5: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.37it/s]\n",
      "Validating Epoch 5: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.12it/s]\n",
      "\n",
      "After 5  Epoch Accuracy= 50.14999999999999 %\n",
      "\n",
      "Training   Epoch 6: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:12<00:00, 161.25it/s]\n",
      "Validating Epoch 6: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.05it/s]\n",
      "\n",
      "After 6  Epoch Accuracy= 59.57 %\n",
      "\n",
      "Training   Epoch 7: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.35it/s]\n",
      "Validating Epoch 7: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.86it/s]\n",
      "\n",
      "After 7  Epoch Accuracy= 65.98 %\n",
      "\n",
      "Training   Epoch 8: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.50it/s]\n",
      "Validating Epoch 8: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 227.27it/s]\n",
      "\n",
      "After 8  Epoch Accuracy= 67.7 %\n",
      "\n",
      "Training   Epoch 9: 100%|███████████████████████████████████████████████████████| 60000/60000 [06:10<00:00, 161.80it/s]\n",
      "Validating Epoch 9: 100%|███████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.56it/s]\n",
      "\n",
      "After 9  Epoch Accuracy= 68.75 %\n",
      "\n",
      "Training   Epoch 10: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:10<00:00, 162.03it/s]\n",
      "Validating Epoch 10: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.78it/s]\n",
      "\n",
      "After 10  Epoch Accuracy= 70.45 %\n",
      "\n",
      "Training   Epoch 11: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.48it/s]\n",
      "Validating Epoch 11: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.13it/s]\n",
      "\n",
      "After 11  Epoch Accuracy= 70.17 %\n",
      "\n",
      "Training   Epoch 12: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.36it/s]\n",
      "Validating Epoch 12: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.19it/s]\n",
      "\n",
      "After 12  Epoch Accuracy= 71.53 %\n",
      "\n",
      "Training   Epoch 13: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.65it/s]\n",
      "Validating Epoch 13: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.94it/s]\n",
      "\n",
      "After 13  Epoch Accuracy= 72.24000000000001 %\n",
      "\n",
      "Training   Epoch 14: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.45it/s]\n",
      "Validating Epoch 14: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.17it/s]\n",
      "\n",
      "After 14  Epoch Accuracy= 71.63000000000001 %\n",
      "\n",
      "Training   Epoch 15: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:11<00:00, 161.54it/s]\n",
      "Validating Epoch 15: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 225.68it/s]\n",
      "\n",
      "After 15  Epoch Accuracy= 73.26 %\n",
      "\n",
      "Training   Epoch 16: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:12<00:00, 161.22it/s]\n",
      "Validating Epoch 16: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 224.69it/s]\n",
      "\n",
      "After 16  Epoch Accuracy= 72.3 %\n",
      "\n",
      "Training   Epoch 17: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:13<00:00, 160.64it/s]\n",
      "Validating Epoch 17: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 223.59it/s]\n",
      "\n",
      "After 17  Epoch Accuracy= 72.36 %\n",
      "\n",
      "Training   Epoch 18: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:17<00:00, 159.06it/s]\n",
      "Validating Epoch 18: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 226.36it/s]\n",
      "\n",
      "After 18  Epoch Accuracy= 72.3 %\n",
      "\n",
      "Training   Epoch 19: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:17<00:00, 158.79it/s]\n",
      "Validating Epoch 19: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 227.93it/s]\n",
      "\n",
      "After 19  Epoch Accuracy= 73.61999999999999 %\n",
      "\n",
      "Training   Epoch 20: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:08<00:00, 162.71it/s]\n",
      "Validating Epoch 20: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 228.10it/s]\n",
      "\n",
      "After 20  Epoch Accuracy= 74.27 %\n",
      "\n",
      "Training   Epoch 21: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:07<00:00, 163.06it/s]\n",
      "Validating Epoch 21: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 227.99it/s]\n",
      "\n",
      "After 21  Epoch Accuracy= 74.92999999999999 %\n",
      "\n",
      "Training   Epoch 22: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:07<00:00, 163.18it/s]\n",
      "Validating Epoch 22: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 227.21it/s]\n",
      "\n",
      "After 22  Epoch Accuracy= 74.76 %\n",
      "\n",
      "Training   Epoch 23: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:06<00:00, 163.67it/s]\n",
      "Validating Epoch 23: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 228.58it/s]\n",
      "\n",
      "After 23  Epoch Accuracy= 74.96000000000001 %\n",
      "\n",
      "Training   Epoch 24: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:06<00:00, 163.81it/s]\n",
      "Validating Epoch 24: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 228.74it/s]\n",
      "\n",
      "After 24  Epoch Accuracy= 75.22 %\n",
      "\n",
      "Training   Epoch 25: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:06<00:00, 163.70it/s]\n",
      "Validating Epoch 25: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 229.55it/s]\n",
      "\n",
      "After 25  Epoch Accuracy= 75.47 %\n",
      "\n",
      "Training   Epoch 26: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:06<00:00, 163.69it/s]\n",
      "Validating Epoch 26: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 228.91it/s]\n",
      "\n",
      "After 26  Epoch Accuracy= 76.14 %\n",
      "\n",
      "Training   Epoch 27: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:06<00:00, 163.69it/s]\n",
      "Validating Epoch 27: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 229.12it/s]\n",
      "\n",
      "After 27  Epoch Accuracy= 77.03 %\n",
      "\n",
      "Training   Epoch 28: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:05<00:00, 164.07it/s]\n",
      "Validating Epoch 28: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 229.92it/s]\n",
      "\n",
      "After 28  Epoch Accuracy= 76.92999999999999 %\n",
      "\n",
      "Training   Epoch 29: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:06<00:00, 163.82it/s]\n",
      "Validating Epoch 29: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 229.80it/s]\n",
      "\n",
      "After 29  Epoch Accuracy= 77.46 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training   Epoch 30: 100%|██████████████████████████████████████████████████████| 60000/60000 [06:06<00:00, 163.77it/s]\n",
      "Validating Epoch 30: 100%|██████████████████████████████████████████████████████| 10000/10000 [00:43<00:00, 229.66it/s]\n",
      "\n",
      "After 30  Epoch Accuracy= 77.48 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------Training NN-------------------------------------------------------\n",
    "cost,accuracy = [],[]\n",
    "plot_epoch = epochs\n",
    "go1 = time.time()\n",
    "for e in range(epochs):\n",
    "    inputs_train_batch, targets_train_bacth = [] , []\n",
    "    Batch_size = Global_Batchsize\n",
    "    BS_loop = Batch_size\n",
    "    Batches1 = X / Batch_size \n",
    "    Batches1 = int(Batches1)\n",
    "    \n",
    "    start = 0\n",
    "    go2 = time.time()\n",
    "    for i in trange(Batches1, desc= 'Training   Epoch {}'.format(e+1)):\n",
    "        inputs_train_batch = inputs_small[start:Batch_size, :]\n",
    "        inputs_train_batch = inputs_train_batch.reshape(28,28)\n",
    "        targets_train_bacth = targets_small[:, start:Batch_size]\n",
    "        Errors_train,final_lr = n.train(inputs_train_batch, targets_train_bacth,i,e)\n",
    "        start = Batch_size\n",
    "        Batch_size = Batch_size + BS_loop\n",
    "        #Cost Calculate\n",
    "        Average_error = numpy.sum(Errors_train,axis=0) / 10\n",
    "        Cost_func =  (1/ (2 * BS_loop)) * (sum(Average_error)**2)\n",
    "        pass\n",
    "    cost.append(Cost_func)\n",
    "    inputs_test_batch, targets_test_bacth = [] , []\n",
    "    Op3 =numpy.empty((10,0))\n",
    "    Batch_size = Global_Batchsize\n",
    "    BS_loop = Batch_size\n",
    "    Batches2 = Y / Batch_size \n",
    "    Batches2 = int(Batches2)\n",
    "    start1 = 0\n",
    "#---------------------------------------------Performance Measure---------------------------------------\n",
    "    for j in trange(Batches2, desc=\"Validating Epoch {}\".format(e+1)):\n",
    "        inputs_test_batch = inputs_test[start1:Batch_size, :]\n",
    "        inputs_test_batch = inputs_test_batch.reshape(28,28)\n",
    "        outputs = n.query(inputs_test_batch)\n",
    "        start1 = Batch_size\n",
    "        Batch_size = Batch_size + BS_loop\n",
    "        Op3=numpy.append(Op3,outputs,axis=1)\n",
    "        pass\n",
    "    correct=0\n",
    "    label = numpy.argmax(Op3,axis=0)\n",
    "    for z in range(Y):\n",
    "        if (test_data_label[z]==label[z]):\n",
    "           correct+=1\n",
    "    Performance = correct / Y\n",
    "    print(\"\\nAfter\", e+1,\" Epoch Accuracy=\", Performance * 100,'%\\n', file=sys.stderr)\n",
    "    accuracy.append(Performance)\n",
    "    end2 = time.time() - go2\n",
    "pass\n",
    "end1 = time.time() - go1\n",
    "max_performance = numpy.max(accuracy)\n",
    "EPOCH = numpy.argmax(accuracy) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------Optimizayion Algorithm & Printing-------------------------------\n",
    "print(30*'%',\"\\n %  Final Result on MNIST %\\n\",30*'%',)\n",
    "print(\"\\nThe Optimization Algorithm was Stochastic G.D on CNN with Momentum ans L.R Decay\\n\")\n",
    "print(\"Time taken for 1 Epoch=\",end2/60,'Minutes')\n",
    "print(\"Total time taken for training=\",end1/3600,'Hours')\n",
    "print('Filter Size=',filter_1,\"No. of hidden Nodes=\",nodes_in_1st_hidden_layer)\n",
    "print (\"Training Set Size=\", X, \"Test Set Size=\", Y, \"Number of Epochs=\", epochs)\n",
    "print(\"Initial L.R=\", learning_rate,\"Final L.R=\", final_lr,\"LR Decay\",decay, \"Momentum=\", beta)\n",
    "print(\"Performance=\", max_performance * 100,'%  at',EPOCH,\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------Plotting------------------------------------------\n",
    "\n",
    "plot = plot_epoch\n",
    "#Learning Curve\n",
    "p = numpy.linspace(1,plot_epoch,plot_epoch)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(p, cost, 'b')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Cost Function')\n",
    "plt.show()\n",
    "#Performance Curve\n",
    "p = numpy.linspace(1,plot_epoch,plot_epoch)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot(p, accuracy, 'g')\n",
    "plt.xlabel('No. of Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
